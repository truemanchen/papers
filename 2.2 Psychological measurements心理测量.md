Demographic information and scores on self-report anxiety and depression instruments can be found in Table 1 for each group. Both groups consisted of individuals of Greek ethnicity, with the exception of 1 patient and 2 healthy participants (immigrants from Albania, Italy, and Germany) who were permanent residents of Greece for over 20 years and competent in reading, speaking and understanding the Greek language. 

**表1为每组的人口统计信息和自我报告焦虑和抑郁工具的得分。两组均由希腊族裔组成，除了1名患者和2名健康参与者（来自阿尔巴尼亚，意大利和德国的移民）以外，他们是希腊的永久居民超过20年，并且能够阅读，口语和理解希腊语。**

Although an effort was made to match the two groups with respect to demographics, the patient group was older and had completed fewer years of formal education than the control group. The two groups did not differ significantly on the percentage of women, which was higher than the percentage of men in both groups and especially among patients (85%) in accordance with the clinical literature [33]. 

**尽管已努力使两组在人口统计学方面匹配，但患者组年龄较大，接受正规教育的时间少于对照组。根据临床文献，两组女性的百分比没有显着差异，高于两组的男性百分比，尤其是患者（85％）[33]。**

The generalizability of the developed method was tested on the AVEC’14 dataset, comprising 300 video recordings from 83 participants obtained in the context of two conditions which are very similar to tasks used in the present study, namely reading a neutral text passage and completing a question-and-answer session with the experimenter.

**在AVEC'14数据集上测试了所开发方法的通用性，该数据集包含来自83个参与者的300个视频记录，这些记录是在两种情况下获得的，这两种情况与本研究中使用的任务非常相似，即阅读中性文字并完成与实验者的问答环节。**



# 2.2 Psychological measurements心理测量

## 2.2.1 Self-reported symptoms of depression and anxiety

Two self-report questionnaires were administered to assess recent and ongoing depression and anxiety symptomatology, namely the Greek adaptations of the Beck Depression Inventory-II (BDI-II) and the State-Trait Anxiety Scale Form Y (trait anxiety subscale).

**进行了两份自我报告问卷，以评估近期和正在进行的抑郁症和焦虑症症状，即希腊改编的贝克抑郁量表-II（BDI-II）和状态-特质焦虑量表Y（特质焦虑量表）。**

 The BDI-II [34] comprises 21 questions, each scored on 0–3, 0–4, or 0–5 point scales. These questions assess emotional and behavioral signs of depression such as body image, hypochondriasis, difficulty working, sleep loss, appetite loss, thoughts of self-punishment, suicidal ideation, and reduced libido. 

**BDI-II [34]包含21个问题，每个问题的得分为0–3、0–4或0–5分。这些问题评估了情绪低落和情绪低落的症状，例如身体形象，软骨病，工作困难，睡眠不足，食欲不振，自我惩罚的想法，自杀意念和性欲降低。**

The higher the score, the more severe the depression symptoms, with scores above 13 points indicating mild to severe depression severity [34]. The STAI-Trait Anxiety Subscale [35] is a self-assessment tool of persistent symptoms (feelings, somatic complaints and behaviors) that are considered as core manifestations of anxiety as a characteristic of the individual.

**得分越高，抑郁症状越严重，得分高于13分表示轻度至重度抑郁严重程度[34]。 STAI-特质焦虑子量表[35]是一种持续性症状（感觉，躯体主诉和行为）的自我评估工具，被认为是个体特征性焦虑的核心表现。**

 It consists of 20 items rated on a 1–4 point scale, with higher total scores indicating higher levels of anxiety. Scores above 39 points indicate clinically significant anxiety symptomatology [35].

**它由20个项目组成，以1-4分制进行评分，总分越高，表示焦虑水平越高。得分超过39分表明临床上有明显的焦虑症状[35]。**

## 2.2.2 Blinded expert annotation盲注专家注释

In addition to clinical diagnosis, BDI-II, and STAI scores, facial expressions and nonverbal cues displayed by each participant over the entire recording session were evaluated independently, based on the recorded video by two psychologists. 

**除了临床诊断，还根据两位心理学家的录制视频，分别评估了每个参与者在整个录制会话中显示的BDI-II和STAI分数，面部表情和非语言提示。**

These ratings aimed at providing a more direct estimate of visual depression manifestations during the experiment. It was surmised that this measure would be more directly comparable to the output of a video processing algorithm than diagnosis, which took place at various times prior to recording, and subjective self-ratings of relevant symptoms. 

**这些评分旨在提供实验过程中视觉抑郁表现的更直接估计。据推测，该措施将比视频处理算法的输出更直接地可比，而不是诊断，该诊断在记录之前的各个时间进行，并对相关症状进行主观自评。**

Each rater was blinded with respect to clinical diagnosis and experimental condition, and was instructed to rate each person on an 0-8 point scale of “depression severity”, with 0 indicating complete absence of visual signs of depression, and 8 indicating the most severe visual signs of depression. 

**每个评估者在临床诊断和实验条件方面均视而不见，并被指示以“抑郁严重程度”的0-8点等级对每个人进行评估，其中0表示完全没有视觉上的抑郁迹象，而8表示最严重的抑郁症视觉上的沮丧迹象。**

Audio playback was turned off during rating, primarily in order to avoid revealing any information regarding the group of the participant, as well as of the task performed. In case of discrepancy between raters equal to or greater than 1.5 points on the scale, a third annotator was employed. 

**在评分过程中，音频回放被关闭，主要是为了避免泄露有关参与者组以及所执行任务的任何信息。如果评分者之间的差异等于或大于1.5分，则使用第三个注释器。**

The final annotation score registered was the grand average of all available annotations during the entire experimental session. Ratings were registered in real-time while viewing the recorded videos in CARMA,4 a software for continuous affect rating and media annotation [36]. Absolute inter-rater agreement was adequate as indicated by an intraclass correlation coefficient (ICC, two-way mixed) of 0.85.

**记录的最终注释分数是整个实验过程中所有可用注释的总和。评分是实时记录的，同时可在CARMA中观看录制的视频，4该软件可实现连续的评价和媒体注释[36]。组间相关系数（ICC，双向混合）为0.85，表明评分者之间的绝对同意是足够的。**

# 2.3 Stimuli and procedures

In designing the study special care was given to: (a) choosing the correct tools to assess depression-related symptoms of participants’ everyday life, and (b) ensuring a wide range of experimental conditions to elicit the targeted emotions in the laboratory.

**在设计研究时，应特别注意：（a）选择正确的工具来评估参与者日常生活中与抑郁相关的症状，以及（b）确保广泛的实验条件来激发实验室中的目标情绪。**

 The latter is based on the assumption that the quality and intensity of such emotions and their related facial expressions will be altered in the presence of significant depression symptomatology [37]. We adopted techniques that involved “human-human interaction” as well as “humancomputer interaction” [38] in both “social” and “non-social” context as described below, while the full protocol is also summarized in Table 2.

**后者基于这样的假设，即在存在明显的抑郁症状时，这种情绪及其相关面部表情的质量和强度会发生变化[37]。我们采用了在“社交”和“非社交”上下文中涉及“人与人之间的交互”以及“人机交互”的技术[38]，如下所述，而完整的协议也总结在表2中。**

## 2.3.2 Social emotion-elicitation setting

In the context of a semi-structured interview with a research assistant (psychologist) participants were asked to describe a positive personal experience in detail and were probed to relive this experience as vividly as possible.

**在与研究助理（心理学家）进行半结构化访谈的情况下，要求参与者详细描述积极的个人经历，并被试探以尽可能生动地重现这种经历。**

 In a similar manner, prior to viewing the sadness clip participants were asked to describe a negative personal experience, which involved sadness or distress. A neutral baseline for the positive/negative experience description comprised reading aloud a 260-word narrative text describing a country excursion.

**以类似的方式，在查看悲伤剪辑之前，要求参与者描述负面的个人经历，其中包括悲伤或痛苦。正面/负面经历描述的中立基线包括大声朗读描述国家旅行的260字叙述文字。**

## 2.3.3 Experimental procedure

As shown in Fig. 1, 5 the participant was seated in front of a PC monitor where stimuli, rating scales and questionnaires were presented and where his/her responses were registered by a camera and a biosignal recording device.

**如图1、5所示，参与者坐在PC监视器的前面，那里展示了刺激，评分量表和问卷，并通过照相机和生物信号记录设备记录了他/她的反应。**

 Two researchers conducted the experiment, one operated the stimulus delivery and recording devices and the second, a psychologist interacted with the participant. The psychologist obtained consent, provided instructions and additional explanations if needed, performed guided relaxation, and conducted the semi-structured interviews.

**两名研究人员进行了这项实验，其中一名操作刺激传递和记录设备，第二名则由心理学家与参与者互动。心理学家获得了同意，必要时提供了指导和其他解释，进行了指导性放松，并进行了半结构式访谈。**

Although a formal clinical evaluation of healthy volunteers was not part of the experimental protocol, the study psychologist was instructed to monitor verbal and nonverbal signs that may be indicative of an undiagnosed mood or anxiety disorder, and administer additional probing questions.

**尽管对健康志愿者的正式临床评估不属于实验方案的一部分，但仍指示研究心理学家监测可能表示未诊断出的情绪或焦虑症的言语和非言语体征，并管理其他探究性问题。**

The order of conditions was fixed to ensure minimal cross-task “emotional contamination”. Conditions intended to produce positive emotions were presented first, followed by a paced relaxation session before the neutral condition was administered. This was followed by conditions designed to induce negative emotions. 

**条件的顺序是固定的，以确保最小的跨任务“情感污染”。首先提出旨在产生积极情绪的条件，然后在给予中性条件之前进行有节奏的放松训练。其次是旨在诱发负面情绪的条件。**

Questionnaires were administered at the end of the sequence to minimize fatigue effects on the video recordings. On two predetermined occasions, the participant was guided through relaxation exercises, involving controlled breathing and brief mindfulness techniques to ensure that emotional states and stress levels returned to base line levels. 

**在序列结束时进行问卷调查，以最大程度地减少对视频记录的疲劳影响。在两次预定的情况下，参与者被引导进行放松运动，包括控制呼吸和简短的正念技巧，以确保情绪状态和压力水平恢复到基线水平。**

This took place prior to the description of positive experience/joy clip and, again, prior to the description of negative experience/sadness clip. Specifically, in the beginning of the protocol (c.f. Table 2 task #4) the participants were instructed by the research assistant on how to breathe in order to relax, while their heart rate and peripheral Blood Volume Pulse (BVP) were monitored through photoplethysmography using a NeXus-10 device (Mind Media, Netherlands). 

**这发生在描述正面体验/快乐剪辑之前，再次发生在描述负面体验/悲伤剪辑之前。具体来说，在实验方案的开始阶段（参见表2任务4），研究助手指导参与者如何呼吸以放松，同时通过光电容积描记法使用以下方法监测他们的心率和外周血容量脉冲（BVP） NeXus-10设备（荷兰Mind Media）。**

Participants were offered a second guided relaxation session immediately following the “Joy” video clip (Task #8 in Table 2) to help them resume baseline levels of emotional and psychophysiological states (i.e., as recorded at the beginning of the experimental session). This was ensured by monitoring BVP on the Nexus-10 device during the breathing exercise. Facial video data used in the present study originated from steps 7–8 and 11–13 of the study protocol, shown in bold. The total duration of the experiment ranged from 60–90 min.

**在“欢乐”视频剪辑（表2中的任务8）之后立即为参与者提供了第二次指导性放松会议，以帮助他们恢复情绪和心理生理状态的基线水平（即，在实验会议开始时记录的水平）。在呼吸运动期间通过监控Nexus-10设备上的BVP可以确保这一点。本研究中使用的面部视频数据来自研究方案的步骤7-8和11-13，以粗体显示。实验的总持续时间为60-90分钟。**

A Point Grey Grasshopper®3 camera was employed to record high-resolution video at a high frame-rate. Camera settings were set to permit future assessment of the impact of recording quality on the algorithm efficiency. Benchmark tests showed that 80 frames per second (fps) and a resolution of 1920 × 1920 pixels was the maximum configuration that the available PC could support. Indirect lighting was applied to the participant’s face to ensure uniform facial illumination and minimize shadows.

**使用Point GrayGrasshopper®3摄像机以高帧频记录高分辨率视频。设置摄像机设置以允许将来评估记录质量对算法效率的影响。基准测试表明，可用PC可以支持的最大配置为每秒80帧（fps）和1920×1920像素的分辨率。间接照明应用于参与者的面部，以确保均匀的面部照明并最大程度地减少阴影。**

# 3 Video processing algorithm

Although three types of signals were recorded from all participants (i.e., visual, audio, and physiological), the present study focused on the video recordings, according to the algorithmic pipeline proposed in [4]. 

**尽管从所有参与者中记录了三种类型的信号（即视觉，音频和生理信号），但根据[4]中提出的算法流程，本研究的重点是视频记录。**

The first step involved extraction of the facial region in each frame and alignment of the successive facial images using OpenFace 2.0,6 which is a widely used open source tool that offers reliable facial  landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation [40].

**第一步涉及使用OpenFace 2.0,6提取每个帧中的面部区域并对齐连续的面部图像，OpenFace 2.0,6是一种广泛使用的开源工具，可提供可靠的面部标志检测，头部姿势估计，面部动作单位识别和眼睛注视估计[40]。**

 Facial landmark detection offers the basis for face alignment and is performed using a Convolutional Experts Constrained Local Model (CE-CLM) [41]. It consists of a Convolutional Experts Network that computes a response map which helps to accurately localize individual landmarks by evaluating the landmark alignment probability at individual pixel locations. An expert layer votes on alignment probability. 

**脸部界标检测为脸部对齐提供了基础，并使用卷积专家约束局部模型（CE-CLM）进行[41]。它由卷积专家网络组成，该卷积专家网络计算响应图，该响应图通过评估各个像素位置处的界标对齐概率来帮助准确定位各个界标。专家层对对齐概率进行投票。**

A Point Distribution Model (PDM) captures landmark shape variations and is used to regularize the shape. The next steps in our pipeline involved feature extraction based on dynamic facial signs, as described below. The complete pipeline of the proposed methodology is illustrated by Fig. 2.

**点分布模型（PDM）捕获地标形状变化，并用于调整形状。我们的产品线的下一步涉及基于动态面部信号的特征提取，如下所述。图2说明了所提出方法的完整流程。**

Signs of depression are dynamic by nature, emanating from motion related patterns (e.g., motor retardation), information which is not conveyed by a static image. Therefore,there is the need to employ an algorithm which considers movement patterns over an image sequence (video versus image processing). 

**抑郁症的征兆本质上是动态的，源于与运动有关的模式（例如，运动迟缓），这些信息不是静态图像所传达的。因此，需要采用一种算法，该算法考虑图像序列上的运动模式（视频与图像处理）。**

Hereby, dynamic facial signs were represented through the motion history image (MHI), and feature extraction was based on this derived image. The latter is a gray scale image, where white pixels correspond to the most recent movements in the video, intermediate gray scale values correspond to less recent movements, and black pixels to the absence of movement. MHIs efficiently encode dynamic behavior over a longer period of time into a static image. 

**从而，通过运动历史图像（MHI）来表示动态面部表情，并且基于此派生图像进行特征提取。后者是灰度图像，其中白色像素对应于视频中的最新移动，中间灰度值对应于视频中的之前的移动，黑色像素对应于不存在移动。 MHI在较长的时间内的动态行为有效地编码为静态图像。**

Moreover, they capture motion flow, as well as the actual moving parts/regions in the video, while being sensitive to the direction of motion, a characteristic which is essential in facial expression analysis. Finally, they register the history of temporal changes in each pixel, thus keeping spatial relationships intact (e.g., eye positions) [42]. Therefore, they have been commonly successfully employed in motion analysis [43], human action recognition [44] as well as in facial action recognition from videos [45].

**此外，它们捕获运动流以及视频中的实际运动部分/区域，同时对运动方向敏感，这是面部表情分析中必不可少的特征。最后，它们记录每个像素中时间变化的历史，从而保持完整的空间关系（例如，眼睛位置）[42]。因此，它们已普遍成功地用于运动分析[43]，人体动作识别[44]以及视频中的面部动作识别[45]。**

The MHI H, with a resolution equal to that of the aligned faces, was computed based on an update function Ψ (x, y) as follows:

**根据更新函数Ψ（x，y）计算出分辨率等于对齐面的分辨率的MHI H，如下所示：**

Feature extraction, explained in some detail at the following subsection, was performed on the MHI pseudo-images (rather than the original video frames). An example of the resulting visualization of the appearance-based features is illustrated in Fig. 3.

**在以下小节中详细解释的特征提取是对MHI伪图像（而不是原始视频帧）执行的。基于外观的特征的可视化示例如图3所示。**

## 3.2 Feature extraction

The next step involved constructing meaningful appearance based descriptors to exploit intensity and texture based attributes. Two appearance-based descriptors were employed, Local Binary Patterns (LBP) and Histogram of Oriented Gradients (HOG), as implemented in MATLAB. 

**下一步涉及构造有意义的基于外观的描述符，以利用基于强度和纹理的属性。如MATLAB中所实现的，使用了两个基于外观的描述符：局部二进制模式（LBP）和定向梯度直方图（HOG）。**

LBP [46] entails dividing the image into partially overlapping cells. Each pixel of the cell is compared to its neighbors to produce a binary value (pattern). The resulting descriptor is a histogram which represents the occurrence of different patterns. LBP combines aspects of statistical and structural texture representation. 

**LBP [46]需要将图像划分为部分重叠的单元格。将单元的每个像素与其相邻像素进行比较，以产生二进制值（图案）。生成的描述符是一个直方图，表示不同模式的出现。 LBP结合了统计和结构纹理表示的各个方面。**

As shown in [47], it can be treated as a special case of a multi-dimensional co-occurrence statistic, a common statistical texture measure. Considering the structural representation, LPBs efficiently encode texture primitives such as spots, flat areas, edges, edge ends, curves etc. [47]. 

**如[47]所示，可以将其视为多维共现统计的一种特例，这是一种常见的统计纹理度量。考虑到结构表示，LPB有效地编码纹理基元，例如斑点，平坦区域，边缘，边缘末端，曲线等[47]。**

For the proposed work the rotation invariant LBP was selected, which for two sets of {radius, neighborhood}, produces two feature vectors of size 1 × 59 for {1, 8} and size 1 × 243 for {2, 16}.

**对于拟议的工作，选择了旋转不变LBP，它对两组{半径，邻域}产生两个特征向量，{1，8}的大小为1×59，{2，16}的大小为1×243**

 The two sets of parameters function supplementing one another, with {1, 8} corresponding to micro-movements, while {2, 16} to movements of larger scale. HOG [48] entails counting gradient orientations in a dense grid. Each image is divided into uniform and nonoverlapping cells, the weighted histogram of binned gradient orientations for each cell is computed, and subsequently combined to form the final feature vector. 

**两组参数相互补充，{1，8}对应微动，而{2，16}对应较大的运动。 HOG [48]要求计算密集网格中的梯度方向。将每个图像分为均匀的和不重叠的单元格，计算每个单元格的合并梯度方向的加权直方图，然后合并以形成最终的特征向量。**

The output corresponds to the concatenated individual histograms resulting in a single spatial HOG histogram. HOG descriptors are very popular in object detection tasks, and can be utilized in encoding facial regions [49], a property we are exploiting with the aim to add regional information to the features.

**输出对应于串联的单个直方图，从而生成单个空间HOG直方图。 HOG描述符在对象检测任务中非常流行，并且可以用于对面部区域进行编码[49]，这是我们正在开发的旨在向特征中添加区域信息的属性。**

Additional features were obtained using the pre-trained Convolutional Neural Network (CNN) architecture developed by the Visual Geometry Group (VGG), University of Oxford, and previously tested by the winning 2014 ImageNet Large Scale Visual Recognition Challenge (ILSVRC) [50]. 

**使用由牛津大学视觉几何学小组（VGG）开发的预先训练的卷积神经网络（CNN）架构获得了额外的功能，该架构先前已获胜的2014 ImageNet大规模视觉识别挑战赛（ILSVRC）进行了测试[50]。**

Relying on multiple convolutional layers, VGG is suitable for describing both textural and appearance-related features and was selected among other pre-trained networks providing a good trade-off between high depth/complexity versus performance while bearing in mind that this work does not target image classification tasks such the ILSVRC.

**依靠多个卷积层，VGG适合于描述纹理和外观相关的特征，并且是在其他预训练网络中选择的，提供了高深度/复杂度与性能之间的良好折衷，同时请记住，这项工作并不针对图像分类任务，例如ILSVRC。**

 We used two CNNs, VGG-16 consisting of 13 convolutional and 3 fully connected layers, and VGG-19 with 16 convolutional and 3 fully connected layers. The MHI image, with pixel values ranging from 0-255, was normalized by subtracting the mean pixel value. The input to VGG (a fixed-size 224×224 MHI image) passed through a stack of convolutional layers, with very small filters who had a receptive field of size 3×3 to capture the notion of left/right, up/down, and center. 

**我们使用了两个CNN，VGG-16由13个卷积层和3个完全连接的层组成，而VGG-19具有16个卷积层和3个完全连接的层。通过减去平均像素值对像素值为0-255的MHI图像进行归一化。 VGG的输入（固定尺寸的224×224 MHI图像）通过一叠卷积层，带有非常小的滤波器，它们的接收场大小为3×3，以捕获左/右，上/下，和中心。**

The convolution stride was fixed to 1 pixel and the spatial padding of a convolutional layer ensured that the input spatial resolution was preserved after convolution (i.e., padding was 1 pixel for 3×3 convolutional layers). Certain convolutional layers were followed by spatial pooling in five max-pooling layers. 

**卷积步幅固定为1个像素，并且卷积层的空间填充可确保在卷积后保留输入的空间分辨率（即3×3个卷积层的填充为1个像素）。某些卷积层之后是五个最大池化层中的空间合并。**

Max-pooling was performed, while a stack of convolutional layers (which had a different depth in different architectures) was followed by three fully connected layers. The final layer was the soft-max layer. The configuration of the fully connected layers was the same in all networks. All hidden layers used the Rectified Linear Unit (ReLU) activation function [51]. 

**进行了最大池化，而一堆卷积层（在不同体系结构中具有不同的深度）之后是三个完全连接的层。最后一层是soft-max层。在所有网络中，完全连接的层的配置均相同。所有隐藏层均使用整流线性单位（ReLU）激活功能[51]。**

Both VGG-16 and VGG-19 were employed in the MATLAB model which was trained on a subset of the ImageNet database (from ILSVRC). Visualization of different activations is illustrated in Fig. 4 and Fig. 5.

**VGG-16和VGG-19均在MATLAB模型中使用，该模型在ImageNet数据库（来自ILSVRC）的子集上进行了训练。图4和图5显示了不同激活的可视化。**

# 3.3 Dimensionality reduction

Principal Component Analysis (PCA) was employed next on all descriptors for dimensionality reduction. PCA is one of the most popular methods for this purpose, and is based on the linear transformation of the original feature vector, into a set of principal components, resulting in uncorrelated data in the new space.

**接下来，在所有描述符上使用主成分分析（PCA）来降低维数。 PCA是用于此目的的最受欢迎的方法之一，它基于将原始特征向量线性变换为一组主成分，从而在新空间中产生不相关的数据。**

 For a dataset of N samples with M features PCA identifies an M × M coefficient matrix (component loadings) that maps each data vector from the original space to a new space of M principal components. Solutions with 5, 10, 20, 40, 45, and 50 principal components were tested separately. The number of principle components to be extracted was chosen empirically based on relative performance

**对于具有M个特征的N个样本的数据集，PCA标识了一个M×M系数矩阵（分量加载），该矩阵将每个数据矢量从原始空间映射到M个主分量的新空间。具有5、10、20、40、45和50个主要成分的解决方案分别进行了测试。根据相对性能凭经验选择要提取的主成分数**

# 3.4 Classification and regression

The efficiency of the extracted features as indices of participant emotional characteristics and state was assessed using Support Vector Machines (SVM) to address binary classification and Support Vector Regression (SVR) for outcome score prediction. 

**使用支持向量机（SVM）评估提取的特征作为参与者情绪特征和状态指标的效率，以解决二进制分类和支持向量回归（SVR）的结果得分预测问题。**

The chosen methods are well-established in the machine learning literature [52]. In the present work we use Linear SVMs due to their simplicity, interpretability, training efficiency, and the relatively low number of features used (principal components identified by PCA). 

**所选的方法在机器学习文献中得到了很好的确立[52]。在本工作中，由于线性SVM的简单性，可解释性，训练效率以及所使用的功能（由PCA识别的主要组件）相对较少，因此我们使用它们。**

The main purpose of SVM is to find an optimal hyperplane that discriminates the samples of the two classes in the feature space. The best hyperplane in a given SVM model corresponds to the one with the largest margin (“distance”) between the two classes as defined by the selected support vectors i.e., the data points that are closer to the separating hyperplane. 

**SVM的主要目的是找到一个最佳超平面，以区分特征空间中两个类别的样本。给定的SVM模型中最好的超平面对应于由所选支持向量（即，更靠近分离的超平面的数据点）定义的两类之间具有最大边距（“距离”）的平面。**

SVR is the extension of SVM for handling regression tasks. The performance of the SVM models was assessed with the following complementary measures: Cohen’s Kappa [53], F1 score, accuracy, precision, and recall were used for the classification tasks, whereas Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) were used for the regression models. Model accuracy was calculated from the following confusion matrix:

**SVR是SVM的扩展，用于处理回归任务。通过以下补充措施评估了SVM模型的性能：Cohen的Kappa [53]，F1得分，准确性，精确度和召回率用于分类任务，而平均绝对误差（MAE）和均方根误差（RMSE） ）用于回归模型。根据以下混淆矩阵计算模型的准确性：**

# 3.5Cross-validation

The evaluation of the proposed algorithm entailed LeaveOne-Subject-Out (LOSO) cross-validation. This process was repeated as many times as the number of participants in the dataset (rather than samples); each time all samples of the given participant were held out from the training set and then used for testing.

**对所提出算法的评估需要使用LeaveOne-Subject-Out（LOSO）交叉验证。重复此过程的次数与数据集中参与者（而不是样本）的数量相同。每次从训练集中提供给定参与者的所有样本，然后用于测试。**

 Cross-validation was performed separately in gender-independent and gender-dependent modes [10]. In the gender-dependent mode both training and testing of the model was conducted with features derived from either male or female participants.

**交叉验证分别以性别独立和性别依赖模式进行[10]。在性别依赖模式下，使用来自男性或女性参与者的特征对模型进行训练和测试。**

# 3.6 Analytic strategy

The second aim (development and testing of an algorithmic pipeline) was pursued by comparing model performance against (a) group labels and (b) continuous assessment of depression symptomatology. During preliminary evaluation of the classification model the performance for high versus low BDI-II scores revealed very low performance (see Table S1), which further encouraged the already intended tests on continuous assessment, supported also by literature [4].

**第二个目标（算法管道的开发和测试）是通过将模型性能与（a）组标签和（b）抑郁症症状的连续评估进行比较而实现的。在对分类模型进行初步评估时，高BDI-II评分与低BDI-II评分的表现均显示出非常低的表现（请参见表S1），这进一步鼓励了已经打算进行的连续评估测试，并得到了文献的支持[4]。**

 The third aim of the study involved applying the best performing model, developed in the context of the 2nd aim to perform continuous assessment of depression symptomatology, on the AVEC’14 dataset in order to evaluate the generalizability of the proposed methodology. To pursue the fourth aim of the study we evaluated the relative performance of the proposed pipeline against selfrated depression symptoms, anxiety symptoms, and expert ratings of overt facial manifestations of depression (continuous assessment models).

**该研究的第三个目标涉及在AVEC 14数据集上应用最佳表现模型，该模型是在第二个目标的背景下开发的，用于对抑郁症症状学进行连续评估，以评估所提出方法的通用性。为了实现研究的第四个目标，我们评估了拟议中的管道针对自评抑郁症状，焦虑症状和抑郁症明显面部表现的专家评级（连续评估模型）的相对性能。**

# 4.1 Psychological measures

As expected, self-reported depression (BDI-II) and anxiety (STAI) scores, as well as the percentage of persons scoring in the clinically significant range on each scale were significantly higher in the patient group. Moreover, the average BDI-II and STAI scores obtained by the control group were well within the range of previous reports from normative samples of Greek speaking young adults [34,54]. 

**正如预期的那样，患者组中自我报告的抑郁（BDI-II）和焦虑（STAI）得分以及在临床上有意义的范围内得分的人的百分比在每个量表上均显着更高。此外，对照组获得的BDI-II和STAI的平均分数完全在以前的报告中，这些报告来自希腊年轻人的规范样本[34,54]。**

There was, however, some evidence that the present sample was somewhat biased toward reporting frequent/more severe anxiety-related symptoms: (a) the percentage of participants in the control group reporting at least mild anxiety symptoms (51.1%) was considerably higher than the corresponding percentage reporting depression symptoms (15.6%), (b) patients reported moderate/severe anxiety symptomatology as frequently as mood disturbances (70.0%).

**但是，有一些证据表明，本样本偏向于报告频繁/更严重的焦虑相关症状：（a）至少报告有轻度焦虑症状的对照组参与者的百分比（51.1％）大大高于相应的报告抑郁症状的百分比（15.6％），（b）患者报告的中度/重度焦虑症状与情绪障碍一样频繁（70.0％）。**

 It should be noted, however, that based on the semi-structured interview with the study psychologist, none of the participants in the control group met criteria for a mood or anxiety disorder

**但是，应该注意的是，根据对研究心理学家的半结构化访谈，对照组中没有人符合情绪或焦虑症的标准**

Self-reported values of depression and anxiety symptoms (STAI and BDI-II scores, respectively) were strongly cross-correlated, as expected (r = .768). The degree of association between self-report measures and expert judgments of depression signs based on visual cues alone was in the moderate range, albeit considerably weaker, given the diverse nature of the two sets of measures. 

**自我报告的抑郁和焦虑症状值（分别为STAI和BDI-II评分）与预期高度相关（r = .768）。鉴于两组措施的性质不同，自我报告措施与仅基于视觉提示的抑郁征兆专家判断之间的关联度处于中等范围，尽管相当弱。**

The slightly higher association between STAI and expert ratings (r = 0.527), as compared to the correlation between expert ratings and BDIII (r = 0.424), may simply reflect the tendency of human raters to focus on manifest signs of psychological distress rather than depression per se

**与专家评级和BDIII之间的相关性（r = 0.424）相比，STAI和专家评级之间的关联性更高（r = 0.527），可能仅反映了人类评级者倾向于关注心理困扰而不是抑郁的明显迹象的趋势。**

The dispersion of BDI-II, STAI, and expert ratings of depression is shown in Fig. 6, revealing that the best separation between the two groups was achieved by expert ratings of depression relying solely on facial expressions. This impression was confirmed by Receiver Operating Characteristic Curve (ROC) analyses, revealing sensitivity/specificity estimates of 70/85% for BDI-II (at the recommended cut-off of 13/14 points), 75/50% for STAI (at the recommended cutoff of 39/40 points), and 85/90% for expert ratings (at the optimal cutoff of 3.4/3.5 points on the 0-8 point scale).

**BDI-II，STAI和抑郁症专家评分的分布如图6所示，表明两组患者之间的最佳分离是通过仅依靠面部表情的抑郁症专家评分实现的。接受者操作特征曲线（ROC）分析证实了这一印象，显示出对BDI-II的灵敏度/特异性估计为70/85％（建议的截止值为13/14点），对于STAI的敏感性/特异性估计为75/50％（在建议的截止值为39/40分），专家评级为85/90％（在0-8分的标准下，最佳截止值为3.4 / 3.5分）。**

# 4.2 Model development and testing for depression assessment

The relative performance of various feature extraction and cross-validation schemes was first assessed on the current dataset against BDI-II scores (i.e., the only common continuous variable with the AVEC’14 dataset). Three participants did not complete one of the tasks, each for different reasons and a different task, bringing the total number of recordings to 322 (out of a possible of 325: 65 participants × 5 tasks).

**首先在当前数据集上针对BDI-II评分（即，AVEC’14数据集唯一的常见连续变量）评估了各种特征提取和交叉验证方案的相对性能。三名参与者未完成一项任务，每项任务都有不同的原因和任务，使记录总数达到322（可能是325：65名参与者×5个任务）。**

 The proposed methodology was tested across all tasks, and for each condition and gender mode separately. Performance of the categorical assessment models for high versus low BDI-II scores was very low when compared to previous tests [12], as indicated by F1 scores < 56.4% (see Table S1). Classification models against clinical diagnosis and expert judgment-based groupings performed comparably (F1 = 61.5% and 57.9%, respectively). 

**所提出的方法论已在所有任务中进行了测试，并分别针对每种情况和性别模式进行了测试。与以前的测试相比，BDI-II得分高与低的分类评估模型的性能非常低[12]，F1得分<56.4％（参见表S1）表明。针对临床诊断和基于专家判断的分组的分类模型具有可比性（F1分别为61.5％和57.9％）。**

Accordingly, the 3rd and 4th specific aims of the study were addressed via continuous assessment models. Among continuous assessment models, the best performance was achieved by VGG-19 features derived from the passage reading condition in gender-independent mode (RMSE/MAE = 10.54/7.86; see Table 3). 

**因此，通过连续评估模型解决了研究的第三和第四特定目标。在连续评估模型中，通过性别独立模式下的段落阅读条件得出的VGG-19功能获得了最佳性能（RMSE / MAE = 10.54 / 7.86；请参阅表3）。**

In this run, there were only two false negative cases (the model underestimated self-reported depression severity for two participants who scored > 55 points on BDI-II). In gender-specific mode our method relying on HOG features produced very similar results. The same feature extraction and cross-validation scheme was applied to the AVEC’14 data from the comparable condition (“Northwind”: passage reading for continuous assessment of BDI-II scores).

**在这一轮中，只有两个假阴性病例（该模型低估了两个参与者的自我报告的抑郁严重程度，他们在BDI-II上得分> 55分）。在特定于性别的模式下，我们基于HOG功能的方法产生了非常相似的结果。相同的特征提取和交叉验证方案被用于来自可比较条件的AVEC'14数据（“ Northwind”：段落阅读以连续评估BDI-II分数）。**

 Results indicated similar performance (RMSE=10.74, MAE=8.91 on the Development set, and RMSE=11.45, MAE=9.92 on the Test set; see Table 4). In relation to the results of previously reported approaches using the AVEC dataset the proposed method

**结果表明性能相似（在开发组上，RMSE = 10.74，MAE = 8.91；在测试组上，RMSE = 11.45，MAE = 9.92；请参见表4）。关于先前报告的使用AVEC数据集的方法的结果，提出的方法**

# 4.3 Assessment of model specificity for depression versus anxiety

Model performance was evaluated for continuous assessment of BDI-II, STAI, and expert rating scores in both gender-dependent and gender-independent modes. The bestperforming scheme in terms on both RMSE and MAE among those listed in Table 3 was the gender-independent model conducted on video recordings from the passage reading condition. Relative model performance can be evaluated by examining log-transformed differences between actual and predicted STAI scores using the following formula:

**对模型性能进行了评估，以持续评估BDI-II，STAI和专家评级分数，包括性别相关和性别独立模式。在表3中列出的在RMSE和MAE方面表现最佳的方案是根据段落阅读条件对视频记录进行的性别无关模型。可以通过使用以下公式检查实际和预测的STAI分数之间的对数转换差异来评估相对模型的性能：**

where Anorm is the normalized observed value and Pnorm the normalized predicted value. Figure 8 also includes a Bland-Altman plot [55] that helps to compare the agreement between the observed and predicted STAI score. It was created using the following formula:

**其中Anorm是归一化的观测值，Pnorm是归一化的预测值。图8还包括一个Bland-Altman图[55]，它有助于比较观察到的STAI分数与预测的STAI分数之间的一致性。它是使用以下公式创建的：**

In auxiliary tests we run SVRs trained on data from all but one condition and tested on the remaining condition. Overall, RMSE values were slightly higher across various features than for the best-performing (passage reading) condition. Moreover, classification models against participants groupings based on STAI and expert rating scores performed poorly as well (see Table S1).

**在辅助测试中，我们运行受SVR训练的SVR，这些SVR接受了除一种情况以外的所有数据的训练，并在其余条件下进行了测试。总体而言，各种功能的RMSE值都比表现最佳（通过读取）的条件略高。此外，基于STAI和专家评分的针对参与者分组的分类模型也表现不佳（参见表S1）。**

# 5.1 Algorithm generalization

The proposed algorithmic pipeline produced comparable results in predicting BDI-II scores in two diverse datasets in terms of sample characteristics (clinical and cultural background) and video image quality, as outlined in Table 5. The robustness of the proposed algorithm is further supported by the fact that the AVEC dataset consisted of pre-extracted dynamic facial landmarks whereas the present dataset consisted of raw video recordings. Besides the direct comparison of results across the two datasets, it is difficult to establish common grounds between the present and previous work due to important methodological differences. For instance, a recent report of depression severity prediction using the Pittsburgh dataset relied on serial video recordings [56] whereas a single measurement was available in the current study. In other reports deep learning was employed for both algorithm training and tuning comparing [13,16] whereas in the current work this technique was used solely for feature extraction.

# 5.2 Algorithm specificity

Given that the proposed pipeline produced considerably better prediction of STAI as compared to either BDI-II scores or expert ratings of depression in the current dataset, one may surmise that it is primarily sensitive to facial features more closely associated with self-reported anxiety (STAI scores). 

**鉴于与当前数据集中的BDI-II评分或抑郁症专家评分相比，拟议中的管道对STAI的预测要好得多，因此可以推测，它主要对与自我报告的焦虑症（STAI）密切相关的面部特征敏感分数）。**

This pattern was present across experimental conditions, thus likely reflected relatively characteristic, dynamic facial signs, although best performance was achieved with video recordings obtained during an emotionally and cognitively non-challenging condition (i.e., reading a neutral passage). It should be noted, however, that best results (prediction of STAI or BDI-II scores) were obtained with video recordings from the neutral, passage-reading task in gender-independent mode. 

**尽管在情绪和认知上没有挑战性的条件下（例如，阅读中性段落）获得的视频记录获得了最佳表现，但这种模式在实验条件下均存在，因此可能反映了相对特征的动态面部表情。但是，应该注意的是，中立的，不分性别的段落阅读任务的录像带获得了最佳结果（对STAI或BDI-II分数的预测）。**

As shown in the Bland-Altman plot of Fig. 8, the most notable failure involved underestimation of STAI scores, given the higher associated clinical risk. Among the four patients in this category, two suffered from severe depression and were treated with high doses of anti-depressants, which may have affected the dynamics of facial expressions. Another patient spoke Greek as a second language and experienced some difficulty in reading the text, while the fourth patient also experienced some difficulty in reading due to reduced visual acuity.

**如图8的Bland-Altman图所示，考虑到较高的相关临床风险，最明显的失败是对STAI分数的低估。在该类别的四名患者中，有两名患有严重的抑郁症，并接受了高剂量的抗抑郁药治疗，这可能影响了面部表情的动态。另一位患者说希腊语作为第二语言，在阅读文本时遇到了一些困难，而第四位患者由于视力下降而在阅读方面也遇到了困难。**

Clinical diagnosis of depression did not emerge as a robust outcome variable in binary classification schemes. In part this finding may be attributed to the considerable overlap between the two study groups on self-reported depression symptomatology (BDI-II scores) and expert-rated facial signs of depression as shown in Fig. 6. 

**在二元分类方案中，抑郁症的临床诊断并未作为可靠的结果变量出现。这一发现可能部分归因于两个研究组在自我报告的抑郁症症状（BDI-II评分）和专家评定的抑郁症面部征象之间的大量重叠，如图6所示。**

The fact that the patients were not treatment-naïve may in part account for this overlap by allowing cases who had responded adequately to treatment and experienced significant remission of depression symptoms to be included in the clinical group. Moreover, none of the participants in the control group met criteria for a mood or anxiety disorder (based on the clinical interview conducted as part of the experiment); 

**患者未接受过治疗的事实可以部分归因于这种重叠，方法是将对治疗有充分反应并且抑郁症状明显缓解的患者纳入临床组。此外，对照组的参与者均未达到情绪或焦虑症的标准（基于作为实验一部分的临床访谈）；**

yet they reported elevated symptoms of trait anxiety on STAI. As a result the overlap between the two clinical groups on STAI scores was even more extensive than the overlap on BDI-II and experts ratings of depression. Given that the proposed algorithm appears to be primarily sensitive to overt signs of anxiety symptoms this extensive overlap may have contributed to the poor classification results of the proposed algorithm against clinical diagnosis of depression.

**但是他们报道了STAI上特质焦虑的症状升高。结果，两个临床组在STAI评分上的重叠甚至比BDI-II和抑郁症专家评分上的重叠更为广泛。考虑到所提出的算法似乎对焦虑症状的明显体征敏感，这种广泛的重叠可能导致了所提出的算法对抑郁症临床诊断的不良分类结果。**

# 5.3 Algorithm development and performance

A notable finding of the present study concerns the relatively poor binary classification performance of our algorithm with respect to all four outcome measures (including clinically acceptable cut-off on STAI scores). For clinical purposes, however, methods such as the one developed here are not intended as standalone diagnostic systems; 

**本研究的显着发现涉及相对于所有四个结果度量（包括STAI分数的临床可接受的临界值），我们算法的二进制分类性能相对较差。但是，出于临床目的，此处开发的方法之类的方法并不旨在作为独立的诊断系统。**

they are evaluated as decision support tools providing tentative recommendations for further clinical exploration. In addition, the relatively poor classification performance may reflect the continuous nature of depression symptomatology. This is likely reflected in a corresponding continuous variation of facial motion dynamic patterns across patients rendering continuous assessment more appropriate. 

**它们被评估为决策支持工具，为进一步的临床探索提供了初步建议。此外，相对较差的分类性能可能反映出抑郁症状的连续性。这很可能反映在患者之间面部运动动态模式的相应连续变化中，从而使连续评估更为合适。**

This problem is compounded by the potential moderating role of clinical factors that may affect the intensity and quality of facial expression of depression symptoms (e.g., negative mood, apathy, helplessness). Such factors include the use of Selective Serotonin Reuptake Inhibitors (SSRIs) medications which may enhance apathy and therefore reduce emotional expressivity, as well as person-specific characteristics (illness-related cognitions and personality characteristics). 

**临床因素的潜在调节作用使这个问题更加复杂，这些临床因素可能会影响抑郁症状（例如负面情绪，冷漠，无助）的面部表情的强度和质量。这些因素包括使用选择性5-羟色胺再摄取抑制剂（SSRIs）药物，该药物可能会增加冷漠感，从而降低情绪表现力，以及特定于人的特征（与疾病有关的认知和人格特征）。**

Another notable finding was that the deep learning approach for feature extraction outperformed the other approaches on most tests (data partitions and experimental conditions). Given that only the generic VGG was employed in the present work, it is highly probable that further training and tuning of the network would significantly improve prediction of depression severity.

**另一个值得注意的发现是，在大多数测试（数据分区和实验条件）上，用于特征提取的深度学习方法优于其他方法。考虑到在目前的工作中仅使用了通用的VGG，很有可能进一步培训和调整网络将显着改善对抑郁严重程度的预测。**

 The highly competitive performance of deep learning has been noted in several research areas. The fact that deep learning does not rely on pre-specified rules, in a manner similar to human cognition, may account for its superiority. The performance of pre-trained deep neuronal networks with a relatively small dataset, such as the one tested in the present work, generates great promises regarding their capacity to provide clinically meaningful results for depression assessment if trained with sufficiently large training datasets.

**深度学习的高度竞争性表现已在多个研究领域得到了关注。深度学习不以类似于人类认知的方式依赖于预先规定的规则这一事实可以解释其优越性。具有相对较小的数据集的预训练深层神经网络的性能（例如，在本工作中进行测试的神经网络），如果通过足够大的训练数据集进行训练，能够为抑郁评估提供具有临床意义的结果，就可以产生很大的希望。**

The experience gained through the experimental tests leads to the conclusion that performance metrics need to be jointly evaluated. The majority of previous reports relied on a single metric (i.e., accuracy) which does not reliably show the capacity of a model, especially in cases of highly unbalanced datasets.

**通过实验测试获得的经验得出结论，即性能指标需要共同评估。以前的大多数报告都依靠单个度量标准（即准确性），该度量标准不能可靠地显示模型的功能，尤其是在高度不平衡的数据集的情况下。**

 By default Cohen’s Kappa encompasses most pertinent information and it is not surprising that the majority of complementary metrics generally agree to Kappa values across studies. Moreover, the F1-score does not necessarily reflect accurate recognition across all classes (i.e., a high F1- score may be associated with good recognition in one class and poor recognition in another) and does not consider the level of chance. Feinstein and Cicchetti [57] however present a more moderate view regarding Kappa. 

**默认情况下，科恩（Cohen）的Kappa包含最相关的信息，并且大多数补充指标通常都同意研究中的Kappa值也就不足为奇了。此外，F1分数不一定反映所有课程的准确识别（即，较高的F1分数可能与一个课程的识别度较高，而在另一课程的识别度较差有关），并且未考虑机会级别。 Feinstein和Cicchetti [57]对Kappa提出了更为温和的看法。**

Their conclusions suggest that Kappa may be overly conservative, as it appears to have serious interpretative problems in the presence of high skew just like accuracy but in the opposite direction. This fact provides an explanation for the very low Kappa values for the categorical assessment models.

**他们的结论表明，Kappa可能过于保守，因为它在存在高偏斜的情况下似乎存在严重的解释性问题，就像准确性一样，但方向相反。这一事实为分类评估模型的非常低的Kappa值提供了解释。**

# 5.4 Data related issues

A note is in place regarding the small but notable superiority of the gender-independent models. This finding, however, may be an artifact of the small sample size which was reduced to marginal levels in the gender-dependent modes, especially when such models were applied to the (relatively few) male participants. 

**关于与性别无关的模型的微小但显着的优越性已有说明。但是，这一发现可能是样本量小的假象，在依赖性别的模式下减少到了边缘水平，尤其是当这种模式应用于（相对较少）男性参与者时。**

Furthermore, although the ratio of depressed to non-depressed participants in the present study does not reflect the population prevalence of depression disordersbased ratios, where the percentage of persons with diagnosis of depression individuals are about 17.5% for women and 14.6% for men [58], still it is better suited for a machine learning problem where the ratio of the different classes cannot be small as it highly impacts the training of the model.

**此外，尽管本研究中抑郁与非抑郁参与者的比例并未反映出以抑郁症为基础的人群患病率，但诊断出患有抑郁症的人中女性约占男性的17.5％，男性约占14.6％[58]。 ]，它仍然更适合于机器学习问题，其中不同类别的比率不能太小，因为它极大地影响了模型的训练。**

 Although not a primary aim of the present study, it was assumed that, in spite of their significant computational cost, high video acquisition specifications (i.e., illumination, image resolution, frame rate) would significantly improve algorithm performance. However, this assumption was not supported by experimental results: binary classification results on the high resolution videos from our dataset performed slightly worse than on the AVEC’14 dataset, while for the continuous assessment they performed at the same level. Therefore, the specifications of the video recordings do not have a great impact on the performance.

**尽管这不是本研究的主要目的，但可以假定，尽管它们的计算成本很高，但高视频采集规范（即照明，图像分辨率，帧速率）仍将显着提高算法性能。但是，实验结果不支持该假设：来自我们数据集的高分辨率视频的二进制分类结果比AVEC’14数据集的表现稍差，而对于连续评估，它们的分类结果却相同。因此，视频记录的规格不会对性能产生很大影响。**

# 5.5 Limitations and future plans

In view of the nature of the problem pursued and the specific requirements of feature extraction and cross-validation methods used in the present study, perhaps the most important limitation concerns overall sample size. The size of the patient group was especially small given the significant variability on clinical characteristics present. 

**考虑到所要解决的问题的性质以及本研究中使用的特征提取和交叉验证方法的具体要求，也许最重要的局限性在于总体样本量。考虑到目前临床特征的显着差异，患者组的规模尤其小。**

These limitations reduced the evaluative power of the proposed algorithm in gender-based mode. Datasets from larger patient samples are paramount in order to properly test the generalizability of the algorithm presented in the current work. This daunting task is particularly important in order to eventually take into account the multitude of factors that may affect overt signs of emotional states and symptoms encompassed in depression, including disease type, severity, and duration, and the impact of treatment (pharmacological and psychotherapeutic). 

**这些限制降低了所提出算法在基于性别的模式下的评估能力。为了正确地测试当前工作中提出的算法的通用性，来自更大患者样本的数据集至关重要。为了最终考虑可能影响抑郁症所包含的明显情绪状态和症状的多种因素，这项艰巨的任务尤其重要，包括疾病类型，严重程度和持续时间以及治疗的影响（药物和心理治疗） 。**

A complementary experimental setup would involve multiple serial measurements (video, biosignals, self- and clinician ratings of psychoemotional state and symptoms) from a relatively smaller group of patients. This approach has produced very promising results in a recent study involving multiple serial video recordings of clinician interviews with patients with depression. 

**补充性实验设置将涉及来自相对较小的一组患者的多个系列测量（视频，生物信号，自我和临床医生对心理情绪状态和症状的评级）。在最近的一项研究中，这种方法产生了非常有希望的结果，该研究涉及临床医生与抑郁症患者的访谈的多个连续视频录像。**

The goal of this approach would be to identify features that show systematic variability over time in relation to clinical outcomes. During the recall and description of positive and negative experience, although the participants were instructed to look at a fixed green mark on the wall, sometimes the participants showed a tendency in changing their head pose to face the interviewer. Although this was not a frequent occurrence its potential impact on algorithm performance was not formally evaluated. 

**该方法的目标是确定与临床结果相关的随时间显示系统变化的特征。在回忆和描述正面和负面经历时，尽管指示参与者注意墙上的固定绿色标记，但有时参与者会表现出改变其头部姿势以面对面试官的趋势。尽管这不是经常发生，但尚未正式评估其对算法性能的潜在影响。**

Moreover, in view of the poor performance of the algorithm against expert judgment of depression, it would be helpful to evaluate the proposed algorithm against expert ratings of anxiety/distress levels derived from visual cues. This test is crucial to address the notion that the proposed algorithm is indeed more sensitive to facial signs of anxiety/distress. 

**此外，鉴于该算法对抑郁症专家判断的性能较差，因此对根据视觉线索得出的焦虑/困扰水平的专家评级对所提出的算法进行评估将很有帮助。该测试对于解决以下观点至关重要：所提出的算法确实对焦虑/困扰的面部症状更敏感。**

It should further be noted that emotional and behavioral symptoms of depression in the present study were assessed using a single self-report scale (BDI-II). Clinician-administered scales, such as the Hamilton Depression Rating Scale (HAM-D) employed in the Pittsburgh University dataset [56], may be more sensitive to clinically significant signs and symptoms of depression.

**还应该注意的是，本研究中抑郁的情绪和行为症状是使用单一的自我报告量表（BDI-II）进行评估的。临床医生管理的量表，例如匹兹堡大学数据集中使用的汉密尔顿抑郁量表（HAM-D）[56]，可能对临床上明显的抑郁症和症状更为敏感。**